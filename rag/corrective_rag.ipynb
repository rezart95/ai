{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da4fd6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key loaded: True\n"
     ]
    }
   ],
   "source": [
    "# CRAG introduces corrective mechanism that assess and refine the quality of retrieved documents.\n",
    "\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "from langchain_core.runnables.graph import CurveStyle, NodeStyles, MermaidDrawMethod\n",
    "from IPython.display import display, HTML, Image\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(f\"OpenAI API key loaded: {openai_key is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "623082bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting document loading...\n",
      "Loaded 3 documents from URLs.\n",
      "Flattened to 3 document items.\n",
      "Split into 187 document chunks.\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"\n",
    "]\n",
    "\n",
    "print(\"Starting document loading...\")\n",
    "# load and split documents\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "print(f\"Loaded {len(docs)} documents from URLs.\")\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "print(f\"Flattened to {len(docs_list)} document items.\")\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=0)\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "print(f\"Split into {len(doc_splits)} document chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29543105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vectorstore...\n",
      "Vectorstore created.\n",
      "Retriever created.\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating vectorstore...\")\n",
    "# add to vectorstore\n",
    "persist_dir = \"./chroma_db\"\n",
    "vectorstore = Chroma.from_documents(\n",
    "    doc_splits,\n",
    "    collection_name=\"crag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=openai_key),\n",
    "    persist_directory=persist_dir,\n",
    ")\n",
    "print(\"Vectorstore created.\")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(\"Retriever created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc79d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: define graders and relevance model\n",
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "retrieval_prompt = ChatPromptTemplate.from_template(\"\"\"You are a grader assessing if a document is relevant to a user's question.\n",
    "                                                    Document: {document}\n",
    "                                                    Question: {question}\n",
    "                                                    Is the document relevant? Answer 'yes' or 'no' \"\"\")\n",
    "\n",
    "retrieval_grader = retrieval_prompt | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).with_structured_output(GradeDocuments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccdf66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: query re-writer\n",
    "class ImproveQuestion(BaseModel):\n",
    "    improved_question: str = Field(description=\"Formulate an improved question\")\n",
    "\n",
    "re_write_prompt = ChatPromptTemplate.from_template(\"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\")\n",
    "query_rewriter = re_write_prompt | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).with_structured_output(ImproveQuestion)\n",
    "\n",
    "# Define prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Use the following context to answer the question:\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "rag_chain = prompt | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1afce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "\n",
    "# step 4: define workflow nodes\n",
    "\n",
    "def retrieve(state):\n",
    "    print(\"[Node] Starting retrieve...\")\n",
    "    question = state[\"question\"]\n",
    "    print(f\"Retrieving documents for question: {question}\")\n",
    "    documents = retriever.invoke(question)\n",
    "    print(f\"Retrieved {len(documents)} documents.\")\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def grade_documents(state):\n",
    "    print(\"[Node] Starting grade_documents...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    filtered_docs = []\n",
    "    web_search_needed = \"No\"\n",
    "    print(f\"Grading {len(documents)} documents for relevance...\")\n",
    "\n",
    "    for i, doc in enumerate(documents):\n",
    "        print(f\"Grading document {i+1}/{len(documents)}...\")\n",
    "        grade = retrieval_grader.invoke({\"question\": question, \"document\": doc.page_content}).binary_score\n",
    "        print(f\"Grade result: {grade}\")\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT--\")\n",
    "            filtered_docs.append(doc)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            web_search_needed = \"Yes\"\n",
    "            print(f\"Web search needed: {web_search_needed}\")\n",
    "            return {\"documents\": filtered_docs, \"question\":question, \"web_search\": web_search_needed}\n",
    "    \n",
    "    print(f\"Filtered to {len(filtered_docs)} relevant documents.\")\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search_needed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71f664d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_query(state):\n",
    "    print(\"[Node] Starting transform_query...\")\n",
    "    question = state[\"question\"]\n",
    "    print(f\"Original question: {question}\")\n",
    "    rewritten_question = query_rewriter.invoke({\"question\": question})\n",
    "    print(f\"Rewritten question: {rewritten_question.improved_question}\")\n",
    "    return {\"question\": rewritten_question.improved_question, \"documents\":state[\"documents\"]}\n",
    "\n",
    "def web_search(state):\n",
    "    print(\"[Node] Starting web_search...\")\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "    \n",
    "    Args:\n",
    "        state(dict): The current graph state\n",
    "        \n",
    "    Returns:\n",
    "        state(dict): Updates documents key with append web results\n",
    "    \"\"\"\n",
    "    print(\"---WEBSEARCH---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    pprint(question+\"\\n\")\n",
    "\n",
    "    print(\"Invoking Tavily search...\")\n",
    "    # perform web search using TavilySearchResults and extract only the 'content' field for Document\n",
    "    search_results = TavilySearchResults(k=3).invoke({\"query\": question})\n",
    "    print(f\"Tavily search returned {len(search_results)} results.\")\n",
    "\n",
    "    # process results to create Document objects only with page_content\n",
    "    web_documents = [\n",
    "        Document(page_content=result[\"content\"]) for result in search_results if \"content\" in result\n",
    "    ]\n",
    "    print(f\"Created {len(web_documents)} web documents.\")\n",
    "\n",
    "    # append web search results to the existing documents\n",
    "    documents.extend(web_documents)\n",
    "    print(f\"Total documents now: {len(documents)}\")\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    print(\"[Node] Starting generate...\")\n",
    "    print(f\"Generating answer for question: {state['question']}\")\n",
    "    generation = rag_chain.invoke({\"context\": state[\"documents\"], \"question\": state[\"question\"]})\n",
    "    print(\"Generation completed.\")\n",
    "    return {\"generation\": generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffe6c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5: Define decision-making logic\n",
    "def decide_to_generate(state):\n",
    "    print(\"[Node] Starting decide_to_generate...\")\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer or re-generate a question\n",
    "    \n",
    "    Args:\n",
    "        state(dict): The current graph state\n",
    "        \n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    web_search = state[\"web_search\"]\n",
    "    print(f\"Web search flag: {web_search}\")\n",
    "    if web_search == \"Yes\":\n",
    "        # all documents have been filtered check_relevance, we will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION TRANSFORM QUERY---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATE ANSWER---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cf43b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building workflow...\n",
      "Compiling workflow...\n",
      "Workflow compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "# step 6: Build the workflow\n",
    "print(\"Building workflow...\")\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\"grade_documents\", decide_to_generate, {\"transform_query\": \"transform_query\", \"generate\": \"generate\"})\n",
    "workflow.add_edge(\"transform_query\", \"web_search\")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "print(\"Compiling workflow...\")\n",
    "app = workflow.compile()\n",
    "print(\"Workflow compiled successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3253f107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49c65033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting workflow execution...\n",
      "[Node] Starting retrieve...\n",
      "Retrieving documents for question: Explain how the different types of memory work?\n",
      "Retrieved 4 documents.\n",
      "Node 'retrieve':\n",
      "{ 'documents': [ Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:'),\n",
      "                 Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:'),\n",
      "                 Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:'),\n",
      "                 Document(metadata={'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'language': 'en', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).')],\n",
      "  'question': 'Explain how the different types of memory work?'}\n",
      "\n",
      "---\n",
      "\n",
      "[Node] Starting grade_documents...\n",
      "Grading 4 documents for relevance...\n",
      "Grading document 1/4...\n",
      "Grade result: yes\n",
      "---GRADE: DOCUMENT RELEVANT--\n",
      "Grading document 2/4...\n",
      "Grade result: yes\n",
      "---GRADE: DOCUMENT RELEVANT--\n",
      "Grading document 3/4...\n",
      "Grade result: yes\n",
      "---GRADE: DOCUMENT RELEVANT--\n",
      "Grading document 4/4...\n",
      "Grade result: no\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "Web search needed: Yes\n",
      "[Node] Starting decide_to_generate...\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "Web search flag: Yes\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION TRANSFORM QUERY---\n",
      "Node 'grade_documents':\n",
      "{ 'documents': [ Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:'),\n",
      "                 Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:'),\n",
      "                 Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:')],\n",
      "  'question': 'Explain how the different types of memory work?',\n",
      "  'web_search': 'Yes'}\n",
      "\n",
      "---\n",
      "\n",
      "[Node] Starting transform_query...\n",
      "Original question: Explain how the different types of memory work?\n",
      "Rewritten question: What are the different types of memory in the human brain, and how does each type function in terms of encoding, storage, and retrieval of information?\n",
      "Node 'transform_query':\n",
      "{ 'documents': [ Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:'),\n",
      "                 Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:'),\n",
      "                 Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:')],\n",
      "  'question': 'What are the different types of memory in the human brain, and '\n",
      "              'how does each type function in terms of encoding, storage, and '\n",
      "              'retrieval of information?'}\n",
      "\n",
      "---\n",
      "\n",
      "[Node] Starting web_search...\n",
      "---WEBSEARCH---\n",
      "('What are the different types of memory in the human brain, and how does each '\n",
      " 'type function in terms of encoding, storage, and retrieval of information?\\n')\n",
      "Invoking Tavily search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\re_zi\\AppData\\Local\\Temp\\ipykernel_171980\\2044079010.py:28: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  search_results = TavilySearchResults(k=3).invoke({\"query\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tavily search returned 5 results.\n",
      "Created 5 web documents.\n",
      "Total documents now: 8\n",
      "Node 'web_search':\n",
      "{ 'documents': [ Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:'),\n",
      "                 Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:'),\n",
      "                 Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:'),\n",
      "                 Document(metadata={}, page_content='1. Gathering: You take in information from your environment.\\n2. Encoding: Your brain translates the information into an easily stored format.\\n3. Storage: Your brain organizes and keeps the translated information in your brain.\\n4. Retrieval: You select and find the stored information you want to remember. [...] Abu Dhabi|Canada|Florida|London|Nevada|Ohio|\\n\\nHome/\\n\\nHealth Library/\\n\\nArticles/\\n\\nMemory\\n\\nAdvertisement\\nAdvertisement\\n\\n# Memory\\n\\nMemory is how your brain processes and stores information so you can access it later. Most memory formation happens in your hippocampus, but the process also involves many other connected brain regions. The three main types of memory are short-term, long-term and sensory memory. Several conditions can affect how well your memory works.\\n\\nAdvertisement [...] Memories form in your hippocampus. This is a part of a larger structure (temporal lobes). The temporal lobe sits behind your temples. You have a temporal lobe (and hippocampus) on each side of your head. They help with memory retrieval.\\n\\nOther parts of your brain participate in memory processes:\\n\\n Long-term memory: Basal ganglia, cerebellum, neocortex, striatum and amygdala.\\n Short-term memory: Prefrontal cortex.\\n Sensory memory: Sensory cortex, parietal lobe and temporal lobe.'),\n",
      "                 Document(metadata={}, page_content=\"Most scientists agree that there are two main types of memory: explicit and implicit. Explicit memory is when you consciously remember something, like facts or events. You have to actively think about an explicit memory. On the other hand, implicit memory means recalling things without effort. An example would be tying your shoes. Your brain just knows how to do it subconsciously. [...] You use different types of memory to remember various kinds of information. Experts don't agree on how to categorize them, but some of the main types include: episodic, semantic. working, prospective, procedural, and sensory. Conditions like Alzheimer's may affect the types of memory in different ways.\\n\\n## Types of Memory FAQs\\n\\nWhat are the 4 types of memory? [...] Experts classify types of memory in several ways. More than four categories have been identified, but four commonly discussed types are:\\n\\n Working memory. This form of short-term memory helps you store information and solve problems.\\n Episodic memory. You need it to remember past events.\\n Semantic memory. It helps you recall facts.\\n Prospective memory. These are memories you need for the future.\\n\\nWhat are the 4 Cs of memory?\"),\n",
      "                 Document(metadata={}, page_content=\"Memory functions through three main stages: encoding (capturing information through your senses), storage (saving it in short-term or long-term memory), and retrieval (accessing stored information when needed)\\n Short-term memory holds information briefly (a few seconds to about a minute) with limited capacity (about 7 items based on 1956 research, though this varies by task), while long-term memory has a much larger capacity and can last years or a lifetime, though retrieval ease varies [...] It's commonly believed there are two types of memory storage: short-term and long-term.\\n\\n#### Short-Term Memory\\n\\nAlso sometimes referred to as working memory, short-term memory is immediate. It's the place where we store information for a limited time (typically a few seconds to about a minute) before it decays and is forgotten. [...] Memory works through three stages: encoding (capturing and processing information through your senses), storage (saving it in short-term or long-term memory), and retrieval (accessing stored information when needed). You can enhance your memory by getting quality sleep, engaging in regular exercise, maintaining a balanced diet, and focusing your attention on what you want to remember.\\n\\nWhen we look at a photograph, what do we see?\\n\\nPeople? A place? Perhaps our most recent selfie?\"),\n",
      "                 Document(metadata={}, page_content='Our memory has three basic functions: encoding, storing, and retrieving information. Encoding is the act of getting information into our memory system through automatic or effortful processing. Storage is retention of the information, and retrieval is the act of getting information out of storage and into conscious awareness through recall, recognition, and relearning. There are various models that aim to explain how we utilize our memory. In this section, you’ll learn about some of these'),\n",
      "                 Document(metadata={}, page_content='Memory is the cognitive ability to retain and use information acquired in the past. There are a variety of different types of memory, including a very brief form of information (sensory memory), memory that is briefly retained and actively utilized (working memory) and different types of long term memory, including explicit or declarative memory and implicit or nondeclarative memory.\\n\\nI hope you find this lesson helpful. If you do, please like and subscribe for more lessons like this one!\\n\\nJJ')],\n",
      "  'question': 'What are the different types of memory in the human brain, and '\n",
      "              'how does each type function in terms of encoding, storage, and '\n",
      "              'retrieval of information?'}\n",
      "\n",
      "---\n",
      "\n",
      "[Node] Starting generate...\n",
      "Generating answer for question: What are the different types of memory in the human brain, and how does each type function in terms of encoding, storage, and retrieval of information?\n",
      "Generation completed.\n",
      "Node 'generate':\n",
      "{ 'generation': 'The human brain has several types of memory, each functioning '\n",
      "                'differently in terms of encoding, storage, and retrieval of '\n",
      "                'information. Here are the main types:\\n'\n",
      "                '\\n'\n",
      "                '1. **Short-Term Memory (STM) or Working Memory**:\\n'\n",
      "                '   - **Function**: This type of memory holds information that '\n",
      "                'we are currently aware of and is necessary for carrying out '\n",
      "                'complex cognitive tasks such as learning and reasoning.\\n'\n",
      "                '   - **Encoding**: Information is encoded through sensory '\n",
      "                'input and is processed for immediate use.\\n'\n",
      "                '   - **Storage**: STM has a limited capacity, typically '\n",
      "                'around 7 items, and retains information for about 20-30 '\n",
      "                'seconds.\\n'\n",
      "                '   - **Retrieval**: Information can be quickly accessed for '\n",
      "                'immediate tasks but may be lost if not transferred to '\n",
      "                'long-term memory.\\n'\n",
      "                '\\n'\n",
      "                '2. **Long-Term Memory (LTM)**:\\n'\n",
      "                '   - **Function**: LTM can store information for extended '\n",
      "                'periods, ranging from days to decades, with a virtually '\n",
      "                'unlimited capacity.\\n'\n",
      "                '   - **Subtypes**:\\n'\n",
      "                '     - **Explicit (Declarative) Memory**: This includes '\n",
      "                'memories that can be consciously recalled, such as facts '\n",
      "                '(semantic memory) and personal experiences (episodic '\n",
      "                'memory).\\n'\n",
      "                '       - **Encoding**: Information is processed and organized '\n",
      "                'for long-term retention.\\n'\n",
      "                '       - **Storage**: Information is stored in various brain '\n",
      "                'regions, including the hippocampus and neocortex.\\n'\n",
      "                '       - **Retrieval**: Accessing explicit memories involves '\n",
      "                'conscious thought and can be triggered by cues or contexts.\\n'\n",
      "                '     - **Implicit (Procedural) Memory**: This type involves '\n",
      "                'skills and routines that are performed automatically, such as '\n",
      "                'riding a bike or typing.\\n'\n",
      "                '       - **Encoding**: Skills are learned through practice '\n",
      "                'and repetition.\\n'\n",
      "                '       - **Storage**: Stored in areas of the brain like the '\n",
      "                'basal ganglia and cerebellum.\\n'\n",
      "                '       - **Retrieval**: Accessing implicit memories occurs '\n",
      "                'unconsciously, often without deliberate effort.\\n'\n",
      "                '\\n'\n",
      "                '3. **Sensory Memory**:\\n'\n",
      "                '   - **Function**: This is a very brief form of memory that '\n",
      "                'captures sensory information from the environment.\\n'\n",
      "                '   - **Encoding**: Sensory input is received and processed '\n",
      "                'almost instantaneously.\\n'\n",
      "                '   - **Storage**: Information is retained for a very short '\n",
      "                'duration (milliseconds to a few seconds).\\n'\n",
      "                '   - **Retrieval**: Retrieval is immediate but limited to the '\n",
      "                'duration of the sensory input.\\n'\n",
      "                '\\n'\n",
      "                'Overall, memory functions through three main stages: '\n",
      "                '**encoding** (capturing information), **storage** (retaining '\n",
      "                'it), and **retrieval** (accessing it when needed). Each type '\n",
      "                'of memory plays a crucial role in how we learn, remember, and '\n",
      "                'perform tasks in our daily lives.'}\n",
      "\n",
      "---\n",
      "\n",
      "Workflow execution completed.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"Explain how the different types of memory work?\"}\n",
    "\n",
    "print(\"Starting workflow execution...\")\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Node '{key}':\")\n",
    "        # print the full state of each node\n",
    "        pprint(value, indent=2, width=80, depth=None)\n",
    "        print(\"\\n---\\n\")\n",
    "print(\"Workflow execution completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f08b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16870a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
